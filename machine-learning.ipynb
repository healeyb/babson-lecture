{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/healeyb/babson-lecture/blob/main/machine-learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs2TA_NnksfS"
      },
      "source": [
        "# Machine Learning\n",
        "\n",
        "Now that you've learned the basics of programming, let's learn the basics of *machine learning*!\n",
        "\n",
        "If you've never heard the term \"machine learning\" before, it's possible you may have instead heard the term **\"artificial intelligence,\"** which is often mistakenly used interchangeably. However, machine learning and AI are not actually the same thing; machine learning is, in fact, a sub-field of artificial intelligence. Machine learning is, by definition, the science of using mathematical formula to make computers perform tasks without explicit instruction. In other words, it is *teaching a machine how to behave by example!*\n",
        "\n",
        "I have often heard people describe \"machine learning\" as complex or impossibly advanced (I blame the news), but in truth anyone can learn to build models (and for certain tasks, you might actually find that it is even easier than solving problems with more traditional programming techniques!) And you don't need to be scared by the implied importance of math; while statistics is central to machine learning, you (the programmer) will rarely need to know how it works under the hood (unless you plan to do some really advanced stuff)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZy6TVvoGYFW"
      },
      "source": [
        "## Terms\n",
        "\n",
        "Since you already know how to write some good code, let's start off by reviewing common terms:\n",
        "\n",
        "*     **Model**: is an encapsulated set of methods ready for execution; for example, a \"gender classification model\" would describe a complete program for identifying the gender of a given name. The word \"model\" is used because a machine learning program is a *representation* of the real world (much like how a model of a building is a miniature replica of a real-life building)\n",
        "\n",
        "*     **Learner**: is a *specific way* to train a model. A learner is *abstract* code that will make it easier for you to train and test a model, and each learner *type* will use a specific statistical formula for doing the work. For example, a Bayesian learner will use Bayes theorum in order to generate a model. There are dozens of different learners, and a multitude of reasons to select one learner over another, but for now you don't need to worry too much about that. As you get more advanced, you'll pick up the pros and cons of each learner type.\n",
        "\n",
        "*     **Annotation**: data that is *annotated* is data that has the *answer*. For example, if you build a model to classify the gender of a given name, you will need some example names that have the correct gender (i.e. `Bryan(male), Bethany(female), Eric(male)`).\n",
        "\n",
        "*     **Training**: before you can *use* a model, you will need to *train* it. You train a model with *annotated* data (as described above). By giving the learner a bunch of examples with the correct answers, it will be able to use those answers to figure out how to create an accurate model.\n",
        "\n",
        "*     **Accuracy**: since a model can never be perfect, we try to figure out just how good a model is by determining it's *accuracy*, which is really just the number of times the model gets it right during *validation* (which is just a fancy name for testing the model after it has been built).\n",
        "\n",
        "*     **Fitness**: the *fitness* of a model describes how well the model is able to work on *new* examples. A good model is able to *generalize* (which means it has learned the correct set of rules to be able to figure out the answer for questions it has never seen before). A model that can generalize well is called *fit*. A model that is only able to provide accurate answers on data that looks very similar to its training data is called *overfit*, whereas a model that is not able to provide accurate answers on any data (including training data) is called *underfit*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHIwLrYpGb5K"
      },
      "source": [
        "## Packages\n",
        "\n",
        "It's also worth noting that basically no machine learning task will need to be wholly built by you, the programmer (unless you are working on some really advanced, bleeding-edge stuff). Rather, you'll use an *open-source* (meaning: free) package to help you do the work!\n",
        "\n",
        "These packages are all responsible for handling the math, making life a lot easier on the programmer!\n",
        "\n",
        "The most common packages that you'll see are:\n",
        "\n",
        "*    **scikit-learn**: a relatively old package written by a Googler (employee at Google) that is designed to standardize methods for building the most popular models. We use scikit a bunch in the examples below, and it runs very fast on most machines!\n",
        "\n",
        "*    **NLTK**: designed to support *natural language* workloads (which means, models designed to work with human languages) was written by the University of Pennsylvania. We use NLTK a bit below, and is comes with some great, free data sets!\n",
        "\n",
        "*    **TensorFlow**: we don't use this package in any examples below, but it is very popular! Written by Google, it was originally designed for *deep learning* (a term we'll discuss more later), but can also be used for a host of other machine learning tasks.\n",
        "\n",
        "*    **Keras**: another package written by a Googler, this package is designed exclusively to build and optimize neural networks. There is a frequent debate in ML circles over whether this package or TensorFlow is best suited for the task (personally, I tend to favor TensorFlow for a host of reasons, but say that to the wrong person and you'll end up in a 2 hour argument!)\n",
        "\n",
        "*    **PyTorch**: another neural network package, this time developed by Facebook. \n",
        "\n",
        "There are more, too, but don't yet worry about the specifics of each, because the underlying concepts are the same for all of them. It's worth knowing the names for the sake of completeness, but it's perfectly acceptable to only use one or two (even for a professional!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJCBvywoZMwl"
      },
      "source": [
        "# Basics\n",
        "\n",
        "So, what, exactly, **is** machine learning?\n",
        "\n",
        "Put simply: machine learning is solving problems with *data*. Traditional programming is called *deterministic*, which just means the problem is solved explicitly by the programmer (you, as the programmer, *determine* the exact path to solution). \n",
        "\n",
        "For example, let's imagine that you have an unordered list of first names and the corresponding gender, and you wanted to sort that big list into two new lists: one for all males and one for all females. You can do this with a simple loop and if/then check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL8Gosm4a0pP"
      },
      "source": [
        "import csv\n",
        "import requests\n",
        "\n",
        "# load the list of names and genders \n",
        "# into memory for sorting\n",
        "\n",
        "localFile = 'names.csv'\n",
        "download = requests.get(\"https://www.healeyengineering.com/files/names.csv\")\n",
        "with open(localFile, 'w') as temp_file:\n",
        "    temp_file.write(download.text)\n",
        "\n",
        "males = []\n",
        "females = []\n",
        "\n",
        "with open(localFile, 'r') as temp_file:\n",
        "  csv_reader = csv.reader(temp_file)\n",
        "\n",
        "  # loop over each item in the list\n",
        "  # and sort into the appropriate list\n",
        "\n",
        "  for line in csv_reader:\n",
        "    if line[1] == \"M\":\n",
        "      males.append(line[0])\n",
        "    elif line[1] == \"F\":\n",
        "      females.append(line[0])\n",
        "\n",
        "print(\"Total males: {}\".format(len(males)))\n",
        "print(\"Total females: {}\".format(len(females)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1qgie03rCPY"
      },
      "source": [
        "That is an example of deterministic programming; each step in the code is clearly designed by the programmer.\n",
        "\n",
        "By contrast, machine learning is considered *non-deterministic* programming, which means the problem won't be explicitly solved by the programmer, but rather *inferred* by having some statistical forumula look carefully at a lot of sample data in order to build a model.\n",
        "\n",
        "For example, let's imagine that after the previous sort, we find that we have yet another set of names, only this set does **not** come with a gender assignment. Since we still need to know the gender in order to properly sort this new list of names, we will have to figure out the gender by some other method; this is an ideal use case for a model, which we can build using the same data that we just sorted. Since we're going to be using machine learning to solve this problem with data, our path is not as straightforward as the deterministic task above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKvOd0zAIu9N"
      },
      "source": [
        "## Four Steps\n",
        "\n",
        "The steps to building a model are:\n",
        "\n",
        "*    **Gather our training and test data**: we need to pull together *annotated* data (in this example, names paired with a gender) and split it up into our *training set* (the name/gender pairs that we'll be using for building the model) and a *test set* (name/gender pairs that will not be used during training, but rather to check the accuracy of the new model once it's built, to make sure it is generalizing well). Figuring out just how to distribute your training and test data is not an exact science, and depends on the size of the overall data set (do you have enough data to train a good model?) and the type of problem you are solving. A good rule of thumb, however, is to set aside 20% of any data set to be used for testing, and leaving 80% of the data set for training the model.\n",
        "\n",
        "*    **Engineer our features**: truthfully, this is a large chunk of what a machine learning engineer actually does for a living. To properly train a model, each piece of *input data* (in this case, the name) needs to be broken down into smaller pieces of data that follow a defined pattern. These smaller pieces are called *features*. For example, a feature of a name could be the first letter of the name, or the last letter, or the length of the name, or the type of the first letter (vowel or consonant), etc. There is no minimum or maximum number of features, the limit is only your imagination. More features doesn't always mean a better model, however; sometimes, too many feature can cause a model to become underfit. Similarly, too few features can cause a model to be overfit. Figuring out the ideal number and type of features can sometimes take a lot of experimentation, but that's what a machine learning engineer is paid to do!\n",
        "\n",
        "*    **Pick a learner**: for this example, we're just doing to use a learner called naive Bayes (specifically, a subtype that uses Bernoulli distribution). In theory, however, we could pick any one of dozens of other learners (i.e. decision trees, logistic regression, etc); or, we could even experiment with using multiple learners. But for now, don't worry too much about which is best.\n",
        "\n",
        "*    **Optimize**: every model can ultimately be made better; there's no such thing as a 100% accurate model. Once you have a functional model, you will want to go back and find ways to *optimize*. Whether that be engineering new features, testing other learners, or changing the size/sample of your training data, every machine learning project is finished with a round of optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X3ycAp3JiqK"
      },
      "source": [
        "## Demo: Gender\n",
        "\n",
        "Let's run through these steps and build a demo of a gender classification model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnUQm3VRwcy5"
      },
      "source": [
        "import csv\n",
        "import requests\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import nltk\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# Step 1: gather our training and test data\n",
        "#   (we will shuffle the data to ensure randomness during training)\n",
        "\n",
        "localFile = 'names.csv'\n",
        "download = requests.get(\"https://www.healeyengineering.com/files/names.csv\")\n",
        "with open(localFile, 'w') as temp_file:\n",
        "  temp_file.write(download.text)\n",
        "\n",
        "samples = []\n",
        "with open(localFile, 'r') as temp_file:\n",
        "  csv_reader = csv.reader(temp_file)\n",
        "  for line in csv_reader:\n",
        "    samples.append([line[0], line[1]])\n",
        "\n",
        "shuffle(samples)\n",
        "\n",
        "test_threshold = int(len(samples)*.2)\n",
        "tests = samples[:test_threshold]\n",
        "training = samples[test_threshold:]\n",
        "\n",
        "# Step 2: setup a method for extracting our features\n",
        "#    (this will be re-used a lot)\n",
        "\n",
        "def extractFeatures(name):\n",
        "  firstLetter = name[0].lower()\n",
        "  firstType = \"vowel\" if firstLetter in [\"a\",\"e\",\"i\",\"o\",\"u\",\"y\"] else \"consonant\"\n",
        "  \n",
        "  lastLetter = name[-1].lower()\n",
        "  lastType = \"vowel\" if lastLetter in [\"a\",\"e\",\"i\",\"o\",\"u\",\"y\"] else \"consonant\"\n",
        "\n",
        "  nameLength = len(name)\n",
        "\n",
        "  return {\n",
        "    \"firstLetter\": firstLetter, \n",
        "    \"firstType\": firstType, \n",
        "    \"lastLetter\": lastLetter, \n",
        "    \"lastType\": lastType, \n",
        "    \"nameLength\": nameLength\n",
        "  }\n",
        "\n",
        "# Step 3a: build our model\n",
        "#    (extract features on both training and test data, then train the model)\n",
        "\n",
        "train_data = []\n",
        "for train in training:\n",
        "  train_data.append(\n",
        "      (extractFeatures(train[0]), train[1])\n",
        "  )\n",
        "\n",
        "test_data = []\n",
        "for test in tests:\n",
        "  test_data.append(\n",
        "      (extractFeatures(test[0]), test[1])\n",
        "  )\n",
        "\n",
        "classifier = nltk.classify.SklearnClassifier(BernoulliNB()).train(train_data)\n",
        "\n",
        "# Step 3b: test our model\n",
        "#     (now that we have a built model, we can run our test set through and see how it did)\n",
        "\n",
        "print(\"Classifier accuracy percent:\", (nltk.classify.accuracy(classifier, test_data))*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_RSN9L70F4b"
      },
      "source": [
        "If you run the above code, you should see an accuracy percentage roughly around 75% (it will fluctuate a little up and down, depending on what data is included in training vs. testing based on the shuffle). What this means is that during testing, the model was able to accurately guess roughly 75% of the test cases. That's pretty good! From here, the job of a machine learning engineer would be to investigate ways to make the model even better (changing the features, playing with the test set, testing other learners, etc); maximizing accuracy is a big part of the job!\n",
        "\n",
        "Now that we've built a good model, let's take it for a spin! Try running some names through it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz3NWMfC0K1_"
      },
      "source": [
        "playing = True\n",
        "while playing:\n",
        "  name = input(\"What name should we check? \")\n",
        "\n",
        "  guess = classifier.classify(extractFeatures(name))\n",
        "  probability = classifier.prob_classify(extractFeatures(name)).prob(guess)\n",
        "  \n",
        "  print(\"The model thinks {} is: {} ({}%)\".format(name, \"Male\" if guess == \"M\" else \"Female\", round(probability * 100)))\n",
        "  print()\n",
        "  \n",
        "  continueCheck = input(\"Do you want to play again? y/n \")\n",
        "  while continueCheck.lower() not in [\"y\",\"n\"]:\n",
        "    continueCheck = input(\"Do you want to play again? y/n \")\n",
        "\n",
        "  if continueCheck == \"n\":\n",
        "    playing = False\n",
        "\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjlzMWMe6Ica"
      },
      "source": [
        "Under the hood, what this model is doing is attempting to identify all the significant statistical correlations between any combination of our features and each of the classification options. Humans could certainly be able to imagine and test some of these rules (perhaps female names end in vowels? We can run through the full training list and check...), but we're slow and are likely to miss some things. Machine learning models are great at quickly checking all possible combinations and building a comprehensive rule set for performing these classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upWLgFTMlL-n"
      },
      "source": [
        "# Types\n",
        "\n",
        "There are ultimately two primary high-level model tasks in machine learning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH-XPgiyJxpD"
      },
      "source": [
        "## Classification\n",
        "\n",
        "The gender model above is representative of one such model type, which is to figure out how to *classify* data; or, put another way: figure out what a piece of data *is* given a specific set of options. The above gender example is a type of classification problem called **binomial classification** (meaning there is only 2 possible answers from which to choose). If a problem has *more* than 2 possible answers, it is called **polynomial classification**. In practice, there is little practical difference between the two, but it's still worth knowing the terminology.\n",
        "\n",
        "There are a host of learners that are specifically good at classification problems, with naive Bayes being one of them (which is the learner we used in the gender model above). It can be easier to achieve higher accuracy with classification since answers are always **bucketed** and less precise, which means they will be grouped into a small set of possibilities. For our gender example, this might not seem to make sense (the only possible unaccounted variation would be names that can be both male or female), but let's imagine that we want to build a new model that would determine the value of a house by determining whether it is of low, medium, or high value. Those three possible combinations can actually have a lot of unaccounted variation; for example, a `$400,000` house and a `$450,000` house might both be classified as *high value*, but that is still nearly `$50,000` in variation. If we need to be more specific, there is another common task in machine learning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BnVCFuy7sYE"
      },
      "source": [
        "## Regression\n",
        "\n",
        "Where classification is attempting to assign a label from a pre-defined set of options, *regression* is a machine learning technique for trying to estimate a specific value (without have to pre-define possible options). Think of regression like traditional math; features become possible values for an unknown equation (like `a + b = c`) and the output will be the raw value of that equation.\n",
        "\n",
        "There are many different types of regression, but one of the most common is **linear** regression, which is a form of regression that attempts to approximate a straight line equation that finds the best approximation of a series of data points. Visually, it looks something like this:\n",
        "\n",
        "![regression example](http://healeyengineering.com/files/regression.png)\n",
        "\n",
        "The power of linear regression is it's ability to handle/ignore outliers and noise. If you are attempted to estimate a value using many potentially interconntected features, you want a method that will effectively manage outliers without spoiling the quality of the estimate.\n",
        "\n",
        "Let's take a look at an example and try to estimate the value of a home in California based on median neighborhood income and the median age of homes in that neighborhood (two important variables in determining home prices):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW-I_W1xuPBG"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Step 1: gather our training and test data\n",
        "#   (we will shuffle the data to ensure randomness during training)\n",
        "\n",
        "localFile = 'california_housing.csv'\n",
        "download = requests.get(\"https://www.healeyengineering.com/files/california_housing.csv\")\n",
        "with open(localFile, 'w') as temp_file:\n",
        "  temp_file.write(download.text)\n",
        "\n",
        "samples = []\n",
        "with open(localFile, 'r') as temp_file:\n",
        "  csv_reader = csv.reader(temp_file)\n",
        "  for line in csv_reader:\n",
        "    samples.append({\n",
        "        \"medianAge\": line[2], \n",
        "        \"totalRooms\": line[3], \n",
        "        \"totalBedrooms\": line[4], \n",
        "        \"population\": line[5],\n",
        "        \"households\": line[6],\n",
        "        \"medianIncome\": line[7],\n",
        "        \"medianValue\": line[8]\n",
        "    })\n",
        "\n",
        "# Step 2: setup a method for extracting our features\n",
        "#    (this will be re-used a lot)\n",
        "\n",
        "def extractFeatures(sample):\n",
        "  return [\n",
        "    sample[\"medianAge\"], sample[\"medianIncome\"]\n",
        "  ]\n",
        "\n",
        "# Step 3a: build our model\n",
        "#    (extract features on both training and test data, then train the model)\n",
        "\n",
        "training = []\n",
        "answers = []\n",
        "for sample in samples:\n",
        "  training.append(\n",
        "      extractFeatures(sample)\n",
        "  )\n",
        "\n",
        "  answers.append(sample[\"medianValue\"])\n",
        "\n",
        "training = np.array(training, dtype=np.float32)\n",
        "answers = np.array(answers, dtype=np.float32)\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(training, answers)\n",
        "\n",
        "# Step 3b: test our model\n",
        "#     (now that we have a built model, we can check for model quality)\n",
        "\n",
        "score = regressor.score(training, answers)\n",
        "\n",
        "print(\"Regressor score:\", score)\n",
        "\n",
        "# Step 4: do some dynamic testing\n",
        "\n",
        "playing = True\n",
        "while playing:\n",
        "  income = input(\"What is an income we should we check? \")\n",
        "  income = float(income) / 10000.0\n",
        "  \n",
        "  age = input(\"What is the average age of the houses, in years? \")\n",
        "  age = float(age)\n",
        "\n",
        "  check = np.array([age, income], dtype=np.float32).reshape(1, -1)\n",
        "\n",
        "  prediction = regressor.predict(check)\n",
        "\n",
        "  print(\"The model thinks that the media home value for areas at that income is: {}\".format(prediction))\n",
        "  print()\n",
        "  \n",
        "  continueCheck = input(\"Do you want to play again? y/n \")\n",
        "  while continueCheck.lower() not in [\"y\",\"n\"]:\n",
        "    continueCheck = input(\"Do you want to play again? y/n \")\n",
        "\n",
        "  if continueCheck == \"n\":\n",
        "    playing = False\n",
        "\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1B3KK8ELUb"
      },
      "source": [
        "So, what is happening here?\n",
        "\n",
        "Much like in the previous classification example, we go through the same basic steps to building our model (prepare our data, identify our features, test our model). However, there is one key difference that you might notice: testing is a *little* different for regression. Since we're not aiming for a tight pre-defined set of answers, we're instead concerned with how *close* the model tends to get to the right answer. Fortunately, most learners will give this to us for free by calculating the *loss* of the build. You can also test the model quality by running some test cases through the finished estimator and check how close the estimates are to the real values.\n",
        "\n",
        "In the above example, we're only passing in 2 of the available features: income and median age of the home. Do you think the model would get more or less accurate if we passed in more of the available features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXbmujX99AOj"
      },
      "source": [
        "# Challenge\n",
        "\n",
        "Now that you understand how to build a model, let's take this knowledge out for a spin and build a new *sentiment* classification model. Sentiment is a very common problem in computational linguistics, wherein we try to figure out exactly how positive or negative (or neutral) a given sentence is, trying to account for how flexible and confusing the human language can be.\n",
        "\n",
        "We've provided the bones for a model below, including some training data. The code below will run, but the model it creates won't be very good (just about 50% accuracy, give or take). It needs a lot of improvement. Try playing with new features (be creative), training/test distributions, and see what it takes to arrive at the best possible model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5zp-cy2brpg"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "import nltk\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# Step 1: gather our training and test data\n",
        "#   (we will shuffle the data to ensure randomness during training)\n",
        "\n",
        "samples = []\n",
        "\n",
        "# first, negative examples\n",
        "\n",
        "localFile = 'sentiment-neg.txt'\n",
        "download = requests.get(\"https://www.healeyengineering.com/files/sentiment/sentiment-neg.txt\")\n",
        "with open(localFile, 'w') as temp_file:\n",
        "  temp_file.write(download.text)\n",
        "\n",
        "with open(localFile, 'r') as file:\n",
        "  lines = file.readlines()\n",
        "  for line in lines:\n",
        "    line = re.sub(r'[^\\x00-\\x7F]','',line)\n",
        "    samples.append([\n",
        "      line.strip(),\n",
        "      'neg'\n",
        "    ])\n",
        "\n",
        "# next, positive examples\n",
        "\n",
        "localFile = 'sentiment-pos.txt'\n",
        "download = requests.get(\"https://www.healeyengineering.com/files/sentiment/sentiment-pos.txt\")\n",
        "with open(localFile, 'w') as temp_file:\n",
        "  temp_file.write(download.text)\n",
        "\n",
        "with open(localFile, 'r') as file:\n",
        "  lines = file.readlines()\n",
        "  for line in lines:\n",
        "    line = re.sub(r'[^\\x00-\\x7F]','',line)\n",
        "    samples.append([\n",
        "      line.strip(),\n",
        "      'pos'\n",
        "    ])\n",
        "\n",
        "shuffle(samples)\n",
        "\n",
        "test_threshold = int(len(samples)*.2)\n",
        "tests = samples[:test_threshold]\n",
        "training = samples[test_threshold:]\n",
        "\n",
        "# Step 2: setup a method for extracting our features\n",
        "#    (this will be re-used a lot)\n",
        "\n",
        "def extractFeatures(sentence):\n",
        "  words = sentence.split()\n",
        "\n",
        "  return {\n",
        "    \"firstWord\": words[0], \n",
        "    \"lastWord\": words[-1]\n",
        "  }\n",
        "\n",
        "# Step 3a: build our model\n",
        "#    (extract features on both training and test data, then train the model)\n",
        "\n",
        "train_data = []\n",
        "for train in training:\n",
        "  train_data.append(\n",
        "      (extractFeatures(train[0]), train[1])\n",
        "  )\n",
        "\n",
        "test_data = []\n",
        "for test in tests:\n",
        "  test_data.append(\n",
        "      (extractFeatures(test[0]), test[1])\n",
        "  )\n",
        "\n",
        "classifier = nltk.classify.SklearnClassifier(BernoulliNB()).train(train_data)\n",
        "\n",
        "# Step 3b: test our model\n",
        "#     (now that we have a built model, we can run our test set through and see how it did)\n",
        "\n",
        "print(\"Classifier accuracy percent:\", (nltk.classify.accuracy(classifier, test_data))*100)\n",
        "\n",
        "# Step 4: do some dynamic testing\n",
        "\n",
        "playing = True\n",
        "while playing:\n",
        "  sentence = input(\"What is a sentence we should we check? \")\n",
        "\n",
        "  guess = classifier.classify(extractFeatures(sentence))\n",
        "  probability = classifier.prob_classify(extractFeatures(sentence)).prob(guess)\n",
        "  \n",
        "  print(\"The model thinks that sentence is: {} ({}%)\".format(guess, round(probability * 100)))\n",
        "  print()\n",
        "  \n",
        "  continueCheck = input(\"Do you want to play again? y/n \")\n",
        "  if continueCheck == \"n\":\n",
        "    playing = False\n",
        "\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxpqTf8t8Xrz"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "There is one unique category of learner within the world of machine learning that is worth talking a little bit about: *neural networks*. In all of the previous examples, the math encapsulated within the learner is used to produce a single consistent output. These models are very powerful, but there are limitations to the scope and scale of such models, and the depth of interconnected rules they can learn.\n",
        "\n",
        "A neural network is different: rather than focusing on a single statistical formula to support a model, a neural network attempts to mimic the methods by which the human brain learns. More specifically, a neural network uses *lots* of little models to create a connected series of hundreds, thousands (or even millions) of small nodes, called neurons, each of which are responsible for going to work on a very small mathematical task before passing their answer on to other sets of neurons until an answer is achieved.\n",
        "\n",
        "This sounds complicated, but it's actually fairly straightforward. Imagine it like this:\n",
        "\n",
        "Let's say you want to build a neural network for deciding whether you should vacation in Paris this year. First, we will need to extract our features (just like any other model). We can imagine a few possible features for this problem:\n",
        "\n",
        "*    Can I fly out in the morning: **yes**\n",
        "*    Can I afford this trip without debt: **no**\n",
        "*    Is there a nice hotel within my budget: **yes**\n",
        "*    Is it going to be warm when I go: **yes**\n",
        "\n",
        "In a neural network, these features become what are known as *input* nodes, and we will give the value of each feature to a series of small models with the responsibility of working on just that feature and figuring out the *weight* of it; or, put another way, how *important* the feature is to the final decision. Based on the weight and the inputs, that neuron can decide whether it believes the answer is **yes** (go to Paris) or **no** (don't go to Paris). These operational nodes are called *hidden* nodes, because they each have a very narrow understanding of the world.\n",
        "\n",
        "To visualize this, imagine each hidden neuron was actually a person, and a group of people are playing a modified game of telephone. Person 1 is responsible *only* for figuring out how important it is that you can fly in the morning; person 2 is responsible for figuring out *only* how important it is that you can travel without debt; etc. Each person will know what every other person knows, but they are only responsible for deciding whether or not to give a thumbs up or down over their specific responsibility.\n",
        "\n",
        "During training, each person has to make a *guess* about how important their area of responsibility is given the training scenario provided. So, person 1 is told that they cannot fly out in the morning, and is also told that person 2 said **yes**, person 3 said **no**, and person 4 said **no**. Person 1 looks at those variables and decides to say **no** (don't go to Paris), and they assign a weight to their answer as 4. Then, after you tally up the 4 answer (1 yes, 3 no) we arrive at a final answer: **no**.\n",
        "\n",
        "Now, imagine that *actual* answer (since, during training, we have the answers available to help train) is **yes**. In this network, each person learns through a process called **backpropagation**, which just means that each person is instructed to adjust their weights based on whether or not they were correct. So, person 2 (who was correct) won't change their weights at all, but person 1, 3 and 4 will all be told to adjust their weights a little bit and try again. This adjustment can be either an increase or decrease in importance (the process by which an increase or decrease is chosen is a little complex, but not really important right now), and once the adjustments have been made, another scenario will be tested.\n",
        "\n",
        "After thousands (or millions) of scenarios are tested (and subsequent weights adjusted), eventually the model will develop a good approximation for the decision making process involved and be able to generalize well.\n",
        "\n",
        "Where this gets interesting is when you adding *multiple layers* of hidden nodes. Neural networks with more than one hidden layer are called **deep neural networks** and is where the term \"deep learning\" comes from. In a deep neural network, the second layer of hidden nodes does **not** know what the original input feature is, but rather only knows the answers provided from the prior layer of nodes. This layering process allows the sequence of nodes to get increasingly more complex and interconnected, which can be powerful if you have a very large set of input nodes.\n",
        "\n",
        "This is a little oversimplified, but you get the general idea.\n",
        "\n",
        "![neural network](http://healeyengineering.com/files/nn.png)"
      ]
    }
  ]
}
